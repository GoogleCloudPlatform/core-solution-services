{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab3c1700-ac45-49d6-994a-de21f38b6ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"../../common/src\")\n",
    "sys.path.append(\"../src\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84285f36-009d-4657-8a2e-b4a2e33fa160",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export PROJECT_ID=\"nasa-genie-dev\"\n",
    "project = \"nasa-genie-dev\"\n",
    "os.environ[\"PROJECT_ID\"] = project\n",
    "os.environ[\"PG_HOST\"] = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e2e9b21-af1c-4a39-8cbd-4de2ac39cf09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: [config/config.py:61 - <module>()] Namespace File not found, setting job namespace as default\n",
      "INFO: [config/model_config.py:341 - set_model_config()] Provider [Vertex] enablement status [True]\n",
      "INFO: [config/model_config.py:361 - set_model_config()] Model [VertexAI-Text] enablement status [False]\n",
      "INFO: [config/model_config.py:341 - set_model_config()] Provider [Vertex] enablement status [True]\n",
      "INFO: [config/model_config.py:361 - set_model_config()] Model [VertexAI-Chat] enablement status [True]\n",
      "INFO: [config/model_config.py:341 - set_model_config()] Provider [Vertex] enablement status [True]\n",
      "INFO: [config/model_config.py:361 - set_model_config()] Model [VertexAI-Chat-Palm2] enablement status [True]\n",
      "INFO: [config/model_config.py:341 - set_model_config()] Provider [Vertex] enablement status [True]\n",
      "INFO: [config/model_config.py:361 - set_model_config()] Model [VertexAI-Chat-Palm2-V2] enablement status [False]\n",
      "INFO: [config/model_config.py:341 - set_model_config()] Provider [Vertex] enablement status [True]\n",
      "INFO: [config/model_config.py:361 - set_model_config()] Model [VertexAI-Gemini-Pro] enablement status [False]\n",
      "INFO: [config/model_config.py:341 - set_model_config()] Provider [Vertex] enablement status [True]\n",
      "INFO: [config/model_config.py:361 - set_model_config()] Model [VertexAI-Gemini-Pro-Vision] enablement status [True]\n",
      "INFO: [config/model_config.py:341 - set_model_config()] Provider [Langchain] enablement status [True]\n",
      "INFO: [config/model_config.py:361 - set_model_config()] Model [VertexAI-Chat-Palm2V2-Langchain] enablement status [False]\n",
      "INFO: [config/model_config.py:341 - set_model_config()] Provider [Vertex] enablement status [True]\n",
      "INFO: [config/model_config.py:361 - set_model_config()] Model [VertexAI-Chat-Palm2-32k] enablement status [False]\n",
      "INFO: [config/model_config.py:341 - set_model_config()] Provider [Langchain] enablement status [True]\n",
      "INFO: [config/model_config.py:361 - set_model_config()] Model [VertexAI-Chat-Palm2-32k-Langchain] enablement status [False]\n",
      "INFO: [config/model_config.py:341 - set_model_config()] Provider [Langchain] enablement status [True]\n",
      "INFO: [config/model_config.py:361 - set_model_config()] Model [VertexAI-Chat-Gemini-Pro-Langchain] enablement status [False]\n",
      "INFO: [config/model_config.py:341 - set_model_config()] Provider [Truss] enablement status [False]\n",
      "INFO: [config/model_config.py:361 - set_model_config()] Model [Truss-Llama2-Chat] enablement status [False]\n",
      "INFO: [config/model_config.py:341 - set_model_config()] Provider [vLLM] enablement status [True]\n",
      "INFO: [config/model_config.py:361 - set_model_config()] Model [vLLM-Gemma-Chat] enablement status [False]\n",
      "INFO: [config/model_config.py:341 - set_model_config()] Provider [ModelGarden] enablement status [True]\n",
      "INFO: [config/model_config.py:361 - set_model_config()] Model [VertexAI-ModelGarden-LLAMA2-Chat] enablement status [False]\n",
      "INFO: [config/model_config.py:341 - set_model_config()] Provider [Langchain] enablement status [True]\n",
      "INFO: [config/model_config.py:361 - set_model_config()] Model [OpenAI-GPT4] enablement status [False]\n",
      "INFO: [config/model_config.py:341 - set_model_config()] Provider [Langchain] enablement status [True]\n",
      "INFO: [config/model_config.py:361 - set_model_config()] Model [OpenAI-GPT4-latest] enablement status [False]\n",
      "INFO: [config/model_config.py:341 - set_model_config()] Provider [Langchain] enablement status [True]\n",
      "INFO: [config/model_config.py:361 - set_model_config()] Model [OpenAI-GPT3.5] enablement status [False]\n",
      "INFO: [config/model_config.py:341 - set_model_config()] Provider [Langchain] enablement status [True]\n",
      "INFO: [config/model_config.py:361 - set_model_config()] Model [Cohere] enablement status [False]\n",
      "INFO: [config/model_config.py:341 - set_model_config()] Provider [LLMService] enablement status [False]\n",
      "INFO: [config/model_config.py:361 - set_model_config()] Model [Llama2cpp] enablement status [False]\n",
      "INFO: [config/model_config.py:341 - set_model_config()] Provider [Vertex] enablement status [True]\n",
      "INFO: [config/model_config.py:361 - set_model_config()] Model [VertexAI-Embedding] enablement status [True]\n",
      "INFO: [config/model_config.py:341 - set_model_config()] Provider [Vertex] enablement status [True]\n",
      "INFO: [config/model_config.py:361 - set_model_config()] Model [VertexAI-Embedding-Vision] enablement status [True]\n",
      "INFO: [config/model_config.py:341 - set_model_config()] Provider [Langchain] enablement status [True]\n",
      "INFO: [config/model_config.py:361 - set_model_config()] Model [OpenAI-Embedding] enablement status [True]\n",
      "INFO: [config/model_config.py:341 - set_model_config()] Provider [Langchain] enablement status [True]\n",
      "INFO: [config/model_config.py:361 - set_model_config()] Model [HuggingFaceEmbeddings] enablement status [True]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lramsey/work/ailp/nasa-core-solution-services/components/llm_service/notebooks/.venv/lib/python3.9/site-packages/langchain_community/llms/__init__.py:166: LangChainDeprecationWarning: `` was deprecated in LangChain 0.0.22 and will be removed in 0.2. An updated version of the  exists in the langchain-community package and should be used instead. To use it run `pip install -U langchain-community` and import as `from langchain_community.chat_models import ChatDatabricks`.\n",
      "  warn_deprecated(\n",
      "/Users/lramsey/work/ailp/nasa-core-solution-services/components/llm_service/notebooks/.venv/lib/python3.9/site-packages/langchain_community/llms/__init__.py:329: LangChainDeprecationWarning: `` was deprecated in LangChain 0.0.22 and will be removed in 0.2. An updated version of the  exists in the langchain-community package and should be used instead. To use it run `pip install -U langchain-community` and import as `from langchain_community.chat_models import ChatMlflow`.\n",
      "  warn_deprecated(\n",
      "/Users/lramsey/work/ailp/nasa-core-solution-services/components/llm_service/notebooks/.venv/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: [config/config.py:112 - <module>()] ENABLE_GOOGLE_LLM = True\n",
      "INFO: [config/config.py:113 - <module>()] ENABLE_OPENAI_LLM = False\n",
      "INFO: [config/config.py:114 - <module>()] ENABLE_COHERE_LLM = False\n",
      "INFO: [config/config.py:115 - <module>()] ENABLE_GOOGLE_MODEL_GARDEN = True\n",
      "INFO: [config/config.py:116 - <module>()] ENABLE_TRUSS_LLAMA2 = False\n",
      "INFO: [config/config.py:117 - <module>()] ENABLE_VLLM_GEMMA = True\n",
      "INFO: [config/config.py:180 - <module>()] Loaded default manifest from /Users/lramsey/work/ailp/nasa-core-solution-services/components/llm_service/notebooks/../src/config/document_manifest.json\n",
      "INFO: [config/vector_store_config.py:44 - <module>()] PG_HOST = [None]\n",
      "INFO: [config/vector_store_config.py:45 - <module>()] PG_DBNAME = [pgvector]\n",
      "INFO: [config/vector_store_config.py:74 - <module>()] PG_HOST is set to [None], not connecting to pgvector\n",
      "INFO: [config/vector_store_config.py:81 - <module>()] Default vector store = [matching_engine]\n",
      "INFO: [config/onedrive_config.py:30 - <module>()] ONEDRIVE_CLIENT_ID = [None]\n",
      "INFO: [config/onedrive_config.py:31 - <module>()] ONEDRIVE_TENANT_ID = [None]\n"
     ]
    }
   ],
   "source": [
    "from services.query.vector_store import LangChainVectorStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "475ef0d9-19fb-4f41-a432-84a862d597b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from common.models import (UserQuery, QueryResult, QueryEngine, QueryDocument,\n",
    "                           QueryReference, QueryDocumentChunk, BatchJobModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "292e558c-309c-477d-8a20-35b8085de353",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('nasa-demo-rbac-qe', 'bAXnpVXD8Txh0Ri3fqes'),\n",
       " ('Test Manifest', '34KBiwqLDqHMG8RYf5kt'),\n",
       " ('nasa-demo-parent-qe', 'PNjLCtT9htwrH9jrvUR7'),\n",
       " ('nasa-demo-shpt', 'GGlB2In4OQnSkOwD9Dy4'),\n",
       " ('nasa-demo-gcs', 'k6A7rBXmEjtzRwaWySmn'),\n",
       " ('nasa-search-integrated-v8', 'fcYZwEthyVcclBwwLpmQ'),\n",
       " ('nasa-search-shpt-v7', 'qZaOrVSGl8rvOsDCjAoi'),\n",
       " ('nasa-search-gcs-v7', 'qqJkXMwHB2Kb0jiJRXEn'),\n",
       " ('nasa-search-integrated-v5', 'nqIU7Oi4qRIOgKllDqsY'),\n",
       " ('nasa-search-shpt-v5', 'bteIzSiaaCd3t8vjzDLw'),\n",
       " ('nasa-search-gcs-v5', 'FZnCyfyWNsk2gcHErMVR')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qe_list = QueryEngine.fetch_all()\n",
    "[(qe.name, qe.id) for qe in qe_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3baaaca0-ad11-444d-a96d-95fa08c5320a",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_engine = QueryEngine.find_by_name('Test Manifest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d532a199-313e-4c77-a40f-de429a136d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "langchain_vector_store = LangChainVectorStore(q_engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dcc35d77-e55d-4c72-bbb7-2b31da13fe85",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'topics': {'$contains': 'plasma'}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example usage\n",
    "filter_str = \"{\\\"topics\\\": {\\\"$contains\\\":\\\"plasma\\\"}}\"\n",
    "parsed_filter = langchain_vector_store.parse_filter(filter_str)\n",
    "parsed_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc059eaa-4f17-4fdc-bd3d-8b50a0888e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_prompt = \"what kinds of stars exist in the galaxy?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "af8752e6-05b4-42a3-b548-1d649db61eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: [services/embeddings.py:65 - get_embeddings()] generating embeddings with VertexAI-Embedding\n",
      "INFO: [services/embeddings.py:149 - generate_embeddings()] generating embeddings for embedding type VertexAI-Embedding\n",
      "INFO: [config/model_config.py:488 - get_provider_value()] Get provider value:\n",
      "INFO: [config/model_config.py:489 - get_provider_value()] provider_id=Vertex\n",
      "INFO: [config/model_config.py:490 - get_provider_value()] model_id=VertexAI-Embedding\n",
      "INFO: [services/embeddings.py:208 - get_vertex_embeddings()] generating Vertex embeddings for 1 chunk(s) embedding model text-embedding-004\n"
     ]
    }
   ],
   "source": [
    "from services import embeddings\n",
    "_, query_embeddings = \\\n",
    "      await embeddings.get_embeddings([query_prompt], q_engine.embedding_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72b60aa4-adb6-4c1c-8d2a-672892ebc1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_embedding = query_embeddings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "817d4d36-5eed-4357-b2bb-17a5e68126c9",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'PostgresVectorStore' has no attribute 'similarity_search_with_score_by_vector'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mlangchain_vector_store\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimilarity_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq_engine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_embedding\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/work/ailp/nasa-core-solution-services/components/llm_service/notebooks/../src/services/query/vector_store.py:445\u001b[0m, in \u001b[0;36mLangChainVectorStore.similarity_search\u001b[0;34m(self, q_engine, query_embedding, query_filter)\u001b[0m\n\u001b[1;32m    443\u001b[0m parsed_filter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparse_filter(query_filter)\n\u001b[1;32m    444\u001b[0m langchain_filter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtranslate_filter(parsed_filter)\n\u001b[0;32m--> 445\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlc_vector_store\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimilarity_search_with_score_by_vector\u001b[49m(\n\u001b[1;32m    446\u001b[0m     embedding\u001b[38;5;241m=\u001b[39mquery_embedding,\n\u001b[1;32m    447\u001b[0m     k\u001b[38;5;241m=\u001b[39mNUM_MATCH_RESULTS,\n\u001b[1;32m    448\u001b[0m     \u001b[38;5;28mfilter\u001b[39m\u001b[38;5;241m=\u001b[39mlangchain_filter\n\u001b[1;32m    449\u001b[0m )\n\u001b[1;32m    450\u001b[0m processed_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_results(results)\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m processed_results\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'PostgresVectorStore' has no attribute 'similarity_search_with_score_by_vector'"
     ]
    }
   ],
   "source": [
    "langchain_vector_store.similarity_search(q_engine, query_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baeaa683-c2f2-4f4a-a4b9-8cd031b37f03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
